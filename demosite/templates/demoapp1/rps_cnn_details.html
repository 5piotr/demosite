{% extends "demoapp1/base.html" %}

{% block title %}
<title>CNN Details</title>
{% endblock %}

{% block body_block %}
{% load static %}

<div class="container">
  <div class="row">
    <div class="col-md-8" style='margin: 0 auto;'>
      <h4 class="my_h4">The Network</h4>
      <p class="my_p">
        This is a convolutional neural network trained to classify images of the Rock Paper Scissors game calls.
        Neural network performing this classification has been trained on a farely small image data set containing
        a total of 2188 similarly oriented images. CNN is composed of following layers:
      </p>
      <dl class="row">
        <dt class="col-sm-3">Convolution</dt>
        <dd class="col-sm-9">128 filters with size 3x3, ReLU activation</dd>
        <dt class="col-sm-3">Pooling</dt>
        <dd class="col-sm-9">size 2x2</dd>
        <dt class="col-sm-3">Convolution</dt>
        <dd class="col-sm-9">64 filters with size 3x3, ReLU activation</dd>
        <dt class="col-sm-3">Pooling</dt>
        <dd class="col-sm-9">size 2x2</dd>
        <dt class="col-sm-3">Convolution</dt>
        <dd class="col-sm-9">32 filters with size 3x3, ReLU activation</dd>
        <dt class="col-sm-3">Pooling</dt>
        <dd class="col-sm-9">size 2x2</dd>
        <dt class="col-sm-3">Flatten</dt>
        <dd class="col-sm-9">&nbsp;</dd>
        <dt class="col-sm-3">Dense</dt>
        <dd class="col-sm-9">128 units, ReLU activation</dd>
        <dt class="col-sm-3">Dropout</dt>
        <dd class="col-sm-9">50%</dd>
        <dt class="col-sm-3">Dense</dt>
        <dd class="col-sm-9">3 units, categorical cross-entropy activation</dd>
      </dl>
      <p class='my_p'>
        Before feeding to the network images are being scaled to 90 x 60 pixels. Color information is beeing retained.
        Despite small training set, the network is having farely good results on examples from outside of the train/test batches:
      </p>
      <img src="{% static 'demoapp1/images/predictor_rotated.jpg' %}" class='img-fluid' alt="">
      <p class='my_p'>
        Those are the best results so far.
      </p>
      <h4 class="my_h4">Alternative approach... that filed</h4>
      <p class='my_p'>
        Natural assumption is that images preprocessed in a smart way, containing only valuable imformaion are better input for training the network.
        It seams that the only valiuable information in that case is the contour of the hand. Below are test results of a network trained
        on images with grayscale and edge recognition filters applied:
      </p>
      <img src="{% static 'demoapp1/images/predictor_rotate_edge_rec.jpg' %}" class='img-fluid' alt="">
      <p class='my_p'>
        The overall results are worse in that case. Despite the filters, the images are still retaining lot of useless noise
        and the edge recognition is still far from ideal. This method still requires more investigation.
      </p>
      <h4 class="my_h4">For best classification results</h4>
      <p class='my_p'>
        All images in the train and test sets have been reoriented to in the following maner. The hand is reaching from the bottom left cornerer
        to the middle of the frame. In ideal scenario the user of this app will use right hand to take a picture of left hand.
        The resulting picture should be well oriented for the classification. Results are also improved when:<br>
        - background is dark or highly contrasting, uniform and non reflective,<br>
        - lighting conditions are uniform/diffussed which is hiding the hand texture,<br>
        - shadows are not visible.
      </p>
    </div>

  </div>

</div>


{% endblock %}
